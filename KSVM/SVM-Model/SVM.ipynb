{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "You want to build the SUPPORT VECTOR MACHINES model from scratch,as the mathematical models behind the data science algorithms really intrigue you.\n",
    "\n",
    "Now you also understand that building mathematical models and understanding them is not an easy task and that you might need assistance.\n",
    "\n",
    "So, you go toyour big data teacher, Mr. Kumar, whow agrees to help you out, but only when you're stuck and have tried your best.\n",
    "\n",
    "He says that he will only guide you with the basics and that you'll be writing the code yourself.\n",
    "\n",
    "However, before starting with the code, he introduces you to SVMs as a briefing exercise.\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "First, Mr. Kumar assumes that  you would’ve accustomed yourself with linear regression and logistic regression algorithms. If not, he suggests you have a look at them before moving on to support vector machine. Support vector machine is another simple algorithm that every machine learning expert should have in his/her arsenal. Support vector machine is highly preferred by many as it produces significant accuracy with less computation power. Support Vector Machine, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in classification objectives.\n",
    "\n",
    "The `objective` of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "#### Hyperplanes and Support Vectors\n",
    "\n",
    "Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. \n",
    "\n",
    "#### Large Margin Intuition\n",
    "\n",
    "In logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function. If the squashed value is greater than a threshold value(0.5) we assign it a label 1, else we assign it a label 0. In SVM, we take the output of the linear function and if that output is greater than 1, we identify it with one class and if the output is -1, we identify is with another class. Since the threshold values are changed to 1 and -1 in SVM, we obtain this reinforcement range of values([-1,1]) which acts as margin.\n",
    "Cost Function and Gradient Updates\n",
    "\n",
    "In the SVM algorithm, we are looking to `maximize the margin between the data points and the hyperplane`. The loss function that helps maximize the margin is `hinge loss`.\n",
    "\n",
    "Mr. Kumar adds that in the SVM classifier, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, the SVM  algorithm has a technique called the kernel trick. The `SVM kernel` is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or outputs you’ve defined.\n",
    "\n",
    "\n",
    "\n",
    "### These are the points that help us build our SVM.\n",
    "\n",
    "The first step to building any model is as usual, importing the libraries.\n",
    "Here, we will be importing numpy and scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import scipy.io as spio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION TO CALCULATE CONFUSING MATRIX, ACCURACY AND FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "Now, Mr. Kumar asks you if you know what a `confusion Matrix` is?\n",
    "\n",
    "You tell him that it is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.\n",
    "It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\n",
    "\n",
    "\n",
    "#### Let’s understand TP, FP, FN, TN in terms of pregnancy analogy.\n",
    "* True Positive:\n",
    "\n",
    "Interpretation: You predicted positive and it’s true.\n",
    "\n",
    "You predicted that a woman is pregnant and she actually is.\n",
    "\n",
    "* True Negative:\n",
    "\n",
    "Interpretation: You predicted negative and it’s true.\n",
    "\n",
    "You predicted that a man is not pregnant and he actually is not.\n",
    "\n",
    "* False Positive: (Type 1 Error)\n",
    "\n",
    "Interpretation: You predicted positive and it’s false.\n",
    "\n",
    "You predicted that a man is pregnant but he actually is not.\n",
    "\n",
    "* False Negative: (Type 2 Error)\n",
    "\n",
    "Interpretation: You predicted negative and it’s false.\n",
    "\n",
    "You predicted that a woman is not pregnant but she actually is.\n",
    "\n",
    "Just Remember, We describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "* Recall: Out of all the positive classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "    TP /TP + FN\n",
    "    \n",
    "\n",
    "* Precision: Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
    "    \n",
    "    TP / TP + FP \n",
    "\n",
    "* Accuracy : Out of all the classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "* F measure: It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n",
    "\n",
    "    2 * Recall * Precision / (Recall + Precision)\n",
    "    \n",
    "    \n",
    "Now, Mr. Kumar asks you to write the code as formuale for the same, by making a function which takes in the target vector actual and predicted values as arguments and builds on the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(y_actual, y_predicted):\n",
    "    tp = 0 #initialising with 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(y_actual)): # iterating\n",
    "        if y_actual[i] > 0:\n",
    "            if y_actual[i] == y_predicted[i]:\n",
    "                tp = tp + 1 # if equal then true positive addition\n",
    "            else:\n",
    "                fn = fn + 1 # if not then false negative\n",
    "        if y_actual[i] < 1:\n",
    "            if y_actual[i] == y_predicted[i]:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "                \n",
    "    cm = [[tn, fp], [fn, tp]] # confusion matrix\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn) #formula\n",
    "    sens = tp/(tp+fn)\n",
    "    prec = tp/(tp+fp)\n",
    "    fm = (2*prec*sens)/(prec+sens)\n",
    "    return cm, accuracy, fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation`: This function returns the confusion matrix, accuracy and the F measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "# Function for Each Kernel\n",
    "\n",
    "Now, Mr Kumar asks you to definet the Functions for each kernel\n",
    "\n",
    "`Linear Kernel` is used when the data is Linearly separable, that is, it can be separated using a single Line. It is one of the most common kernels to be used. It is mostly used when there are a Large number of Features in a particular Data Set. Training a SVM with a Linear Kernel is Faster than with any other Kernel.\n",
    " It is mathematically the dot product of both the arguements.\n",
    " \n",
    "`Polynomial Kernel` is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models\n",
    "Although the RBF (Gaussian) kernel is more popular in SVM classification than the polynomial kernel, the latter is quite popular in natural language processing (NLP) The most common degree is d = 2 (quadratic), since larger degrees tend to overfit on NLP problems.\n",
    "\n",
    "Various ways of computing the polynomial kernel (both exact and approximate) have been devised as alternatives to the usual non-linear SVM training algorithms, including:\n",
    "\n",
    "    full expansion of the kernel prior to training/testing with a linear SVM,[5] i.e. full computation of the mapping φ as in polynomial regression;\n",
    "    basket mining (using a variant of the apriori algorithm) for the most commonly occurring feature conjunctions in a training set to produce an approximate expansion;[6]\n",
    "    inverted indexing of support vectors.[6][1]\n",
    "\n",
    "One problem with the polynomial kernel is that it may suffer from numerical instability: when xTy + c < 1, K(x, y) = (xTy + c)d tends to zero with increasing d, whereas when xTy + c > 1, K(x, y) tends to infinity\n",
    "\n",
    "`Gaussian (RBF) Kernel`:The ‘kernel’ for smoothing, defines the shape of the function that is used to take the average of the neighboring points. A Gaussian kernel is a kernel with the shape of a Gaussian (normal distribution) curve\n",
    "In the standard statistical way, we have defined the width of the Gaussian shape in terms of σ. However, when the Gaussian is used for smoothing, it is common for imagers to describe the width of the Gaussian with another related measure, the Full Width at Half Maximum (FWHM)\n",
    "\n",
    "`How?`\n",
    "\n",
    "* Mr. Kumar tells you to make the linear kernel as a dot product.\n",
    "\n",
    "* The polynomial kernel would also include the polynomial parameter p\n",
    "\n",
    "* The RBF (Gaussian) Kernel would return the following\n",
    "\n",
    "    `np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))`\n",
    "    \n",
    "\n",
    "A `sigma (∑)` is a Summation operator. It evaluates a certain expression many times, with slightly different variables, and returns the sum of all those expressions. \n",
    "\n",
    "`numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)`\n",
    "\n",
    "`Matrix or vector norm.`\n",
    "\n",
    "    This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n",
    "\n",
    "`Parameters`\n",
    "\n",
    "* xarray_like\n",
    "\n",
    "* Input array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.\n",
    "        ord{non-zero int, inf, -inf, ‘fro’, ‘nuc’}, optional\n",
    "\n",
    "* Order of the norm . inf means numpy’s inf object. The default is None.\n",
    "        axis{None, int, 2-tuple of ints}, optional.\n",
    "\n",
    "            If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n",
    "\n",
    "           \n",
    "* keepdimsbool, optional\n",
    "\n",
    "            If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n",
    "\n",
    "            New in version 1.10.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "    \n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation:` We have built the functions to define the kernels.\n",
    "\n",
    "`Story`:\n",
    "\n",
    "Mr. Kumar now tells you to define a class with all the parameters which would contain the following functions:\n",
    "\n",
    "* initial __init__  : This would initialise the object with the default values\n",
    "* svmTrain \n",
    "* svmPredict\n",
    "\n",
    "\n",
    "Mr. kumar would be guiding you through the comments in the main code now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM CLASS WITH TRAIN AND PREDICT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    \n",
    "    def __init__(self, kernel=linear_kernel, tol=1e-3, C=0.1, max_passes=5):\n",
    "        \n",
    "        self.kernel = kernel \n",
    "        self.tol = tol\n",
    "        self.C = C\n",
    "        self.max_passes = max_passes\n",
    "        self.model = dict()\n",
    "    \n",
    "    def svmTrain(self, X, Y):\n",
    "        # Data parameters\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Map 0 to -1\n",
    "        Y = np.where(Y == 0, -1, 1)\n",
    "        \n",
    "        # Variables\n",
    "        alphas = np.zeros((m, 1), dtype=float)\n",
    "        b = 0.0\n",
    "        E = np.zeros((m, 1),dtype=float)\n",
    "        passes = 0\n",
    "        \n",
    "        # Precompute the kernel matrix\n",
    "        if self.kernel == linear_kernel:\n",
    "            print('Precomputing the kernel matrix')\n",
    "            K = X @ X.T\n",
    "        elif self.kernel == gaussian_kernel:\n",
    "            print('Precomputing the kernel matrix')\n",
    "            X2 = np.sum(np.power(X, 2), axis=1).reshape(-1, 1)\n",
    "            K = X2 + (X2.T - (2 * (X @ X.T)))\n",
    "            K = np.power(self.kernel(1, 0), K)\n",
    "        else:\n",
    "            # Pre-compute the Kernel Matrix\n",
    "            # The following can be slow due to lack of vectorization\n",
    "            print('Precomputing the kernel matrix')\n",
    "            K = np.zeros((m, m))\n",
    "            for i in range(m):\n",
    "                for j in range(m):\n",
    "                    x1 = np.transpose(X[i, :])\n",
    "                    x2 = np.transpose(X[j, :])\n",
    "                    K[i, j] = self.kernel(x1, x2)\n",
    "                    K[i, j] = K[j, i]\n",
    "                    \n",
    "        print('Training...')\n",
    "        print('This may take 1 to 2 minutes')\n",
    "\n",
    "        while passes < self.max_passes:\n",
    "            num_changed_alphas = 0\n",
    "            \n",
    "            for i in range(m):\n",
    "\n",
    "                E[i] = b + np.sum( alphas * Y * K[:, i].reshape(-1, 1)) - Y[i]\n",
    "\n",
    "                if (Y[i] * E[i] < -self.tol and alphas[i] < self.C) or (Y[i] * E[i] > self.tol and alphas[i] > 0):\n",
    "                    j = np.random.randint(0, m)\n",
    "                    while j == i:\n",
    "                        # make sure i is not equal to j\n",
    "                        j = np.random.randint(0, m)\n",
    "\n",
    "                    E[j] = b + np.sum(alphas * Y * K[:, j].reshape(-1, 1)) - Y[j]\n",
    "\n",
    "                    # Save old alphas\n",
    "                    alpha_i_old = alphas[i, 0]\n",
    "                    alpha_j_old = alphas[j, 0]\n",
    "\n",
    "                    # Compute L and H by (10) or (11)\n",
    "                    if Y[i] == Y[j]:\n",
    "                        L = max(0, alphas[j] + alphas[i] - self.C)\n",
    "                        H = min(self.C, alphas[j] + alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, alphas[j] - alphas[i])\n",
    "                        H = min(self.C, self.C + alphas[j] - alphas[i])\n",
    "                    if L == H:\n",
    "                        # continue to next i\n",
    "                        continue\n",
    "\n",
    "                    # compute eta by (14)\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
    "                    if eta >= 0:\n",
    "                        # continue to next i\n",
    "                        continue\n",
    "\n",
    "                    # compute and clip new value for alpha j using (12) and (15)\n",
    "                    alphas[j] = alphas[j] - (Y[j] * (E[i] - E[j])) / eta\n",
    "\n",
    "                    # Clip\n",
    "                    alphas[j] = min(H, alphas[j])\n",
    "                    alphas[j] = max(L, alphas[j])\n",
    "\n",
    "                    # Check if change in alpha is significant\n",
    "                    if np.abs(alphas[j] - alpha_j_old) < self.tol:\n",
    "                        # continue to the next i\n",
    "                        # replace anyway\n",
    "                        alphas[j] = alpha_j_old\n",
    "                        continue\n",
    "\n",
    "                    # Determine value for alpha i using (16)\n",
    "                    alphas[i] = alphas[i] + Y[i] * Y[j] * (alpha_j_old - alphas[j])\n",
    "\n",
    "                    # Compute b1 and b2 using (17) and (18) respectively.\n",
    "                    b1 = b - E[i] - Y[i] * (alphas[i] - alpha_i_old) * K[i, j] - Y[j] * (alphas[j] - alpha_j_old) * K[i, j]\n",
    "                    \n",
    "                    b2 = b - E[j] - Y[i] * (alphas[i] - alpha_i_old) * K[i, j] - Y[j] * (alphas[j] - alpha_j_old) * K[j, j]\n",
    "                    \n",
    "                    # Compute b by (19).\n",
    "                    if 0 < alphas[i] and alphas[i] < self.C:\n",
    "                        b = b1\n",
    "                    elif 0 < alphas[j] and alphas[j] < self.C:\n",
    "                        b = b2\n",
    "                    else:\n",
    "                        b = (b1 + b2) / 2\n",
    "                    num_changed_alphas = num_changed_alphas + 1\n",
    "\n",
    "            if num_changed_alphas == 0:\n",
    "                passes = passes + 1\n",
    "            else:\n",
    "                passes = 0\n",
    "\n",
    "            print('....')\n",
    "\n",
    "        print(' DONE! ')\n",
    "\n",
    "        # Save the model\n",
    "        idx = alphas > 0\n",
    "        \n",
    "        self.model['X'] = X[idx.reshape(1, -1)[0], :]\n",
    "        self.model['y'] = Y[idx.reshape(1, -1)[0]]\n",
    "        self.model['kernelFunction'] = self.kernel\n",
    "        self.model['b'] = b\n",
    "        self.model['alphas'] = alphas[idx.reshape(1, -1)[0]]\n",
    "        self.model['w'] = np.transpose(np.matmul(np.transpose(alphas * Y), X))\n",
    "        # return model\n",
    "    \n",
    "    def svmPredict(self, X):\n",
    "        if X.shape[1] == 1:\n",
    "            X = np.transpose(X)\n",
    "\n",
    "        # Dataset\n",
    "        m = X.shape[0]\n",
    "        p = np.zeros((m, 1))\n",
    "        pred = np.zeros((m, 1))\n",
    "        \n",
    "        if self.model['kernelFunction'] == linear_kernel:\n",
    "            p = X.dot(self.model['w']) + self.model['b']\n",
    "            \n",
    "        elif self.model['kernelFunction'] == gaussian_kernel:\n",
    "            # Vectorized RBF Kernel\n",
    "            # This is equivalent to computing the kernel on every pair of examples\n",
    "            X1 = np.sum(np.power(X, 2), axis=1).reshape(-1, 1)\n",
    "            X2 = np.transpose(np.sum(np.power(self.model['X'], 2), axis=1))\n",
    "            K = X1 + (X2.T - (2 * (X @ (self.model['X']).T)))\n",
    "            K = np.power(self.model['kernelFunction'](1, 0), K)\n",
    "            K = np.transpose(self.model['y']) * K\n",
    "            K = np.transpose(self.model['alphas']) * K\n",
    "            p = np.sum(K, axis=1)\n",
    "            \n",
    "        else:\n",
    "            for i in range(m):\n",
    "                prediction = 0\n",
    "                for j in range(self.model['X'].shape[0]):\n",
    "                    prediction = prediction + self.model['alphas'][j] * self.model['y'][j] * self.model['kernelFunction'](np.transpose(X[i,:]), np.transpose(self.model['X'][j,:]))\n",
    "                    \n",
    "                p[i] = prediction + self.model['b']\n",
    "\n",
    "        # Convert predictions into 0 and 1                                                                                                                   \n",
    "        pred[p >= 0] = 1\n",
    "        pred[p < 0] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation`: The svm class returns the predictions, now let's experiemnt our model on some real data.\n",
    "\n",
    "Mr. Kumar tells you that he has two datasets for training and testing the model, but they are as .mat files.\n",
    "So, you will use scipy to load the mat files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING THE SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spio.loadmat('spamTrain.mat')\n",
    "test = spio.loadmat('spamTest.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will be accessing the parameters on the dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.double(train.get('X'))\n",
    "y_train = np.double(train.get('y'))\n",
    "X_test = np.double(test.get('Xtest'))\n",
    "y_test = np.double(test.get('ytest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mr. kumar now asks you to call the class SVM by making an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you call the Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing the kernel matrix\n",
      "Training...\n",
      "This may take 1 to 2 minutes\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      "....\n",
      " DONE! \n"
     ]
    }
   ],
   "source": [
    "model.svmTrain(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.svmPredict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the accuracy of the model. Mr kumar tells you that he had curated data hence the accuracy might be very high on this data but not very good on other real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2722, 1], [5, 1272]], 0.9985, 0.9976470588235294)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix(y_train, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation`: The results are as expected.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "SVM is one of the most popular, versatile supervised machine learning algorithm. It is used for both classification and regression task.But in this thread we will talk about classification task. It is usually preferred for medium and small sized data-set.\n",
    "\n",
    "    The main objective of SVM is to find the optimal hyperplane which linearly separates the data points in two component by maximizing the margin \n",
    "    \n",
    "We have build the SVM from scratch and understood the maths behind it as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
