{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Naive Bayes\n",
    "`Story`\n",
    "\n",
    "You have been studying about machine learning algorithms for a while now, and you want to implement Naive Bayes algorithm from scratch to study the mathematics behind it, and compare it to the efficiency of the Scikit-Learn models we have.\n",
    "\n",
    "You know that `classifier systems` are most popular with spam filtering for emails, collaborative filtering for recommendation engines and sentiment analysis. AI is good with demarcating groups based on patterns over large sets of data.\n",
    "\n",
    "Naive Bayes classifier is based on Bayes’ theorem and is one of the oldest approaches for classification problems.\n",
    "\n",
    "`P(A|B) = P(B|A).P(A) / P(B)`\n",
    "\n",
    "\n",
    "The objective here is to determine the likelihood of an event A happening given B happens.\n",
    "\n",
    "The naive Bayes classifier combines Bayes’ model with decision rules like the hypothesis which is the most probable outcomes.\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "\n",
    "It was initially introduced for text categorisation tasks and still is used as a benchmark.\n",
    "\n",
    "There have been many innovations like Support Vector Machines or KNN over the years in solving the classification problem with more flexibility and smartly. But Naive Bayes classifier can still be competent with enough pre-processed data and has shown great results in medical applications where classification is crucial in diagnosis.\n",
    "\n",
    "\n",
    "### How Good Is NB Classifier For ML\n",
    "\n",
    "The first assumption of a Naive Bayes classifier is that the value of a particular feature is independent of the value of any other feature. Which means that the interdependencies within data are comfortably neglected. Hence the name ‘naive.’\n",
    "\n",
    "A naive Bayes classifier considers every feature to contribute independently to the probability irrespective of the correlations.\n",
    "\n",
    "For `unsupervised` or in more practical scenarios, maximum likelihood is the method used by naive Bayes model in order to avoid any Bayesian methods, which are good in supervised setting.\n",
    "\n",
    "`Gaussian Naive Bayes` classifier where the feature values are assumed to be distributed in accordance with Gaussian distribution. The likelihood of the feature being classified is assumed to be Gaussian.\n",
    "\n",
    "\n",
    "`Multinomial Naive Bayes` classifier considers feature vectors which are representation of the frequencies with which few events have been generated by a multinomial distribution.\n",
    "\n",
    "Whereas, in Bernoulli Naive Bayes approach, features are independent booleans and can be used for binary responses.\n",
    "\n",
    "For example, in document classification tasks, Multinomial NB can be used for a number of times a word appears in the document(frequency). And, Bernoulli NB for classifying whether a word appears or not (a binary YES or NO).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take an example to understand Naive Bayes \n",
    "\n",
    "Imagine two people `Alice and Bob` whose word usage pattern you know. To keep example simple, lets assume that Alice uses combination of three words [love, great, wonderful] more often and Bob uses words [dog, ball wonderful] often.\n",
    "\n",
    "Lets assume you received and anonymous email whose sender can be either Alice or Bob. Lets say the content of email is “I love beach sand. Additionally the sunset at beach offers wonderful view”\n",
    "\n",
    "#### Can you guess who the sender might be?\n",
    "Well if you guessed it to be Alice you are correct. Perhaps your reasoning would be the content has words love, great and wonderful that are used by Alice.\n",
    "\n",
    "Now let’s add a combination and probability in the data we have.Suppose Alice and Bob uses following words with probabilities as show below. Now, can you guess who is the sender for the content : “Wonderful Love.”\n",
    "\n",
    "Now what do you think?\n",
    "\n",
    "If you guessed it to be Bob, you are correct. If you know mathematics behind it, good for you. If not, This is where we apply Bayes Theorem.\n",
    "\n",
    "\n",
    "`P(A|B) = P(B|A).P(A) / P(B)`\n",
    "\n",
    "**It tells us how often A happens given that B happens, written P(A|B), when we know how often B happens given that A happens, written P(B|A) , and how likely A and B are on their own.**\n",
    "\n",
    "    P(A|B) is “Probability of A given B”, the probability of A given that B happens\n",
    "    P(A) is Probability of A\n",
    "    P(B|A) is “Probability of B given A”, the probability of B given that A happens\n",
    "    P(B) is Probability of B\n",
    "\n",
    "* When P(Fire) means how often there is fire, and P(Smoke) means how often we see smoke, then:\n",
    "\n",
    "* P(Fire|Smoke) means how often there is fire when we see smoke.\n",
    "* P(Smoke|Fire) means how often we see smoke when there is fire.\n",
    "\n",
    "So the formula kind of tells us “forwards” when we know “backwards” (or vice versa)\n",
    "\n",
    "Example: If dangerous fires are rare (1%) but smoke is fairly common (10%) due to factories, and 90% of dangerous fires make smoke then:\n",
    "\n",
    "P(Fire|Smoke) =P(Fire) P(Smoke|Fire) =1% x 90% = 9%P(Smoke)10%\n",
    "\n",
    "In this case 9% of the time expect smoke to mean a dangerous fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "Now, you want to build both Supervised and Unsupervised Classifiers using Naive Bayes, and use them on a couple of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How?`\n",
    "\n",
    "The first step is to import the necesarry libraries, and here they would be `random` and `intertools (permutations)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import random\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Function for confusion matrix\n",
    "\n",
    "Now, she asks you if you know how to define the confusion matrix. You remember having done it before as well.\n",
    "\n",
    "\n",
    "`How?`\n",
    "\n",
    "So, you decide to move ahead in the following manner:\n",
    "* Take the data and predicted list as arguments.\n",
    "* Calculate the accuracy of the predicted classes:\n",
    "    Here, you iterate through the data and check if the original and predicted data is the same, if yes, you increase the counter by 1.\n",
    "* Calculate the confusion matrix and print it. Note that you should be using the dictionary properties for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(data, predict_list):\n",
    "    # Calculate the accuracy of predicted classes\n",
    "    count = 0\n",
    "    true_list = []\n",
    "    for i in range(len(data)):\n",
    "        true_list.append(data[i][-1])\n",
    "        if data[i][-1] == predict_list[i]:\n",
    "            count += 1\n",
    "    \n",
    "    # Calculate the confusion matrix of result\n",
    "    class_dict = dict([(line[-1], 0) for line in data])\n",
    "    for k in class_dict.keys():\n",
    "        class_dict[k] = dict([(line[-1], 0) for line in data])\n",
    "    \n",
    "    for i in range(len(true_list)):\n",
    "        class_dict[true_list[i]][predict_list[i]] += 1\n",
    "        \n",
    "    # Print format of confusion matrix\n",
    "    print('{0:>20s}|'.format('Actual\\Predict'), end = '')\n",
    "    for k, v in class_dict.items():\n",
    "        print('{0:>20s}|'.format(k), end='')\n",
    "    print()\n",
    "    for k, v in class_dict.items():\n",
    "        print('{0:>20s}|'.format(k), end='')\n",
    "        for values in v.values():\n",
    "            print('{0:>20d}|'.format(values), end='')\n",
    "        print()\n",
    "    print(\"\\nAccuracy: {0}\\n\".format(count / len(data)))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation`: We have calculated and printed the confusion matrix from the above function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`:\n",
    "    \n",
    "Now, Mayuri asks you if you have opened csv files without pandas before, and you remember doing it for your mathematical models before as well.\n",
    "\n",
    "`How?`\n",
    "So, it is not much work for her to brief you about opening a csv using file handling in python.\n",
    "You create an empty list and append the data in it by iterating through the lines using `readlines()` function.\n",
    "To eliminate the empty spaces, you use `.strip()` built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "\n",
    "file_list = [ 'car.csv', 'hypothyroid.csv']\n",
    "\n",
    "def preprocess(filename):\n",
    "    \"\"\"Read csv file from input filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: name of csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: 2-D array of data from csv file\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    data = []\n",
    "    \n",
    "    for line in file.readlines():\n",
    "        data.append(line.strip().split(','))\n",
    "        \n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation`: This function preprocesses the data and creates a dataframe in the form of a list, and returns the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior and Posterior Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "Mayuri now tells you that you can build both supervised as well as unsupervised Naive Bayes classifiers.\n",
    "\n",
    "Within the field of machine learning, there are two main types of tasks: `supervised, and unsupervised`. The main difference between the two types is that supervised learning is done using a ground truth, or in other words, we have prior knowledge of what the output values for our samples should be. Therefore, the goal of supervised learning is to learn a function that, given a sample of data and desired outputs, best approximates the relationship between input and output observable in the data. Unsupervised learning, on the other hand, does not have labeled outputs, so its goal is to infer the natural structure present within a set of data points.\n",
    "\n",
    "    Supervised learning is typically done in the context of classification, when we want to map input to output labels, or regression, when we want to map input to a continuous output. Common algorithms in supervised learning include logistic regression, naive bayes, support vector machines, artificial neural networks, and random forests. In both regression and classification, the goal is to find specific relationships or structure in the input data that allow us to effectively produce correct output data. Note that “correct” output is determined entirely from the training data, so while we do have a ground truth that our model will assume is true, it is not to say that data labels are always correct in real-world situations. Noisy, or incorrect, data labels will clearly reduce the effectiveness of your model.\n",
    "\n",
    "The most common tasks within unsupervised learning are clustering, representation learning, and density estimation. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels. Some common algorithms include k-means clustering, principal component analysis, and autoencoders. Since no labels are provided, there is no specific way to compare model performance in most unsupervised learning methods.\n",
    "\n",
    "\n",
    "    Unsupervised learning is very useful in exploratory analysis because it can automatically identify structure in data. For example, if an analyst were trying to segment consumers, unsupervised clustering methods would be a great starting point for their analysis. In situations where it is either impossible or impractical for a human to propose trends in the data, unsupervised learning can provide initial insights that can then be used to test individual hypotheses.\n",
    "\n",
    "\n",
    "Naive Bayes can be built for both supervised and unsupervised learning, however it is majorly used for supervised classfication.\n",
    "\n",
    "So, we will build a standard `supervised Naive Bayes model`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How?`\n",
    "\n",
    "We begin by taking the data as input, which we had processed in the earlier functions. The data would be a 2D array from the csv file.\n",
    "\n",
    "This would return the prior and posterior probabilities, which would be used in further functions to make predictions using Supervised NB.\n",
    "\n",
    "She now explains you what are prior and posterior probabilities.\n",
    "\n",
    "`Story`\n",
    "\n",
    "\n",
    "### `What is the prior?`\n",
    "\n",
    "Prior is a probability calculated to express one's beliefs about this quantity before some evidence is taken into account. In statistical inferences and bayesian techniques, priors play an important role in influencing the likelihood for a datum.\n",
    "\n",
    "\n",
    "`Black Swan Paradox`\n",
    "\n",
    "Theory of black swan events is a metaphor that describes an event that comes as a surprise, has a major effect, and is often inappropriately rationalized after the fact with the benefit of hindsight. The term is based on an ancient saying that presumed black swans did not exist – a saying that became reinterpreted to teach a different lesson after black swans were discovered in the wild. \n",
    "\n",
    "Such events will have huge impact while training bayesian classifiers - especially, naive bayes where the product of probabilities just turn 0. To avoid blindly rejecting a data point, we use the\n",
    "prior probability to move ahead. \n",
    "\n",
    "    The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "`For example`, three acres of land have the labels A, B, and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil being found on acre C is one third, or 0.333. But if a drilling test is conducted on acre B, and the results indicate that no oil is present at the location, then the posterior probability of oil being found on acres A and C become 0.5, as each acre has one out of two chances. \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Posterior probability` \n",
    "It is the probability an event will happen after all evidence or background information has been taken into account. It is closely related to prior probability, which is the probability an event will happen before you taken any new evidence into account. You can think of posterior probability as an adjustment on prior probability:\n",
    "\n",
    "* Posterior probability = prior probability + new evidence (called likelihood).\n",
    "\n",
    "A posterior probability is the probability of assigning observations to groups given the data. A prior probability is the probability that an observation will fall into a group before you collect the data. For example, if you are classifying the buyers of a specific car, you might already know that 60% of purchasers are male and 40% are female. If you know or can estimate these probabilities, a discriminant analysis can use these prior probabilities in calculating the posterior probabilities. When you don't specify prior probabilities, Minitab assumes that the groups are equally likely.\n",
    "\n",
    "With the assumption that the data have a normal distribution, the linear discriminant function is increased by ln(pi), where pi is the prior probability of group i. Because observations are assigned to groups by the smallest generalized distance, or equivalently the largest linear discriminant function, the effect is to increase the posterior probabilities for a group with a high prior probability.\n",
    "\n",
    "\n",
    "`What is a Posterior Distribution?`\n",
    "\n",
    "Mayuri goes a step further to explain you that The posterior distribution is a way to summarize what we know about uncertain quantities in Bayesian analysis. It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the “new evidence”). In other words, the posterior distribution summarizes what you know after the data has been observed. The summary of the evidence from the new observations is the likelihood function.\n",
    "\n",
    "Posterior Distribution = Prior Distribution + Likelihood Function (“new evidence”)\n",
    "\n",
    "`Example of a Posterior Probability`\n",
    "\n",
    "As a simple example to envision posterior probability, suppose there are three acres of land with labels A, B and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil in acre C is one-third or 33%. A drilling test is conducted on acre B, and the results indicate that no oil is present at the location. With acre B eliminated, the posterior probability of acre C containing oil becomes 0.5, or 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train_supervised(data):\n",
    "    \"\"\"Build a supervised Naive Bayes model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: 2-D array of data from csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prior_dict: dictionary of prior probability\n",
    "    poste_dict: 3-D dictionary of posterior probability\n",
    "    \"\"\"\n",
    "    # Prior probability\n",
    "    prior_dict = {}\n",
    "    # Posterior probability\n",
    "    poste_dict = {}\n",
    "    \n",
    "    # Calculate prior probability\n",
    "    for line in data:\n",
    "        # Calculate prior count in last column\n",
    "        clas = line[-1]\n",
    "        # Check class exists in dictionary\n",
    "        if clas not in prior_dict:\n",
    "            prior_dict[clas] = 1\n",
    "        else:\n",
    "            prior_dict[clas] += 1\n",
    "    \n",
    "    # Divide with instance number to get prior probability        \n",
    "    for key, value in prior_dict.items():\n",
    "        prior_dict[key] = value / len(data)\n",
    "    \n",
    "    # Calculate posterior probability\n",
    "    for att in range(len(data[0])-1):\n",
    "        clas_dict = {}\n",
    "        for line in data:\n",
    "            # Calculate posterior count in last column\n",
    "            clas = line[-1]\n",
    "            # Check class exists in dictionary\n",
    "            if clas not in clas_dict:\n",
    "                clas_dict[clas] = {}\n",
    "            \n",
    "            # Check attribute exists in dictionary\n",
    "            if line[att] not in clas_dict[clas]:\n",
    "                clas_dict[clas][line[att]] = 1\n",
    "            else:\n",
    "                clas_dict[clas][line[att]] += 1\n",
    "\n",
    "        poste_dict[att] = clas_dict\n",
    "        \n",
    "        # Divide with instance number to get posterior probability\n",
    "        for cla in prior_dict.keys():\n",
    "            sum_value = sum(clas_dict[cla].values())\n",
    "            for key, value in clas_dict[cla].items():\n",
    "                clas_dict[cla][key] = value / sum_value\n",
    "\n",
    "    return prior_dict, poste_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation` : This function returns prior_dict: dictionary of prior probability, and poste_dict: 3-D dictionary of posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "Here, Mayuri asks you to take the data, the prior dictionary and the poterior dictionary as arguments and build the prediction function.\n",
    "\n",
    "* You are asked to create a list to store predicted values.\n",
    "* Next, you find the length of the data rows.\n",
    "* Iterate through the data, and then iterate through key and items of the prior_dictionary.\n",
    "* Ignore missing value marked with ?\n",
    "* Check attribute have corresponding posterior probability\n",
    "* Epsilon smoothing if no value exists\n",
    "* Append the class with most possible probability value\n",
    "* Return the predicted list\n",
    "\n",
    "\n",
    "`Hint`: Epsilon smoothing. replace zero with a trivially small non-zero number, typically called ε ε < 1/n for n instances. Compare number of ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for a set of instances, based on a trained model \n",
    "def predict_supervised(data, prior_dict, poste_dict):\n",
    "    \"\"\"Predict the class based on a trained model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prior_dict: dictionary of prior probability\n",
    "    poste_dict: 3-D dictionary of posterior probability\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predict_list: list of predicted classes\n",
    "    \"\"\"\n",
    "    predict_list = []\n",
    "    sum_value = len(data)\n",
    "    \n",
    "    for line in data:\n",
    "        predict_dict = {}\n",
    "        for key, value in prior_dict.items():\n",
    "            # Inital prior probability\n",
    "            predict_dict[key] = value\n",
    "            for index in range(len(line)-1):\n",
    "                att = line[index]\n",
    "                # Ignore missing value marked with ?\n",
    "                if att != '?':\n",
    "                    # Check attribute have corresponding posterior probability\n",
    "                    if att in poste_dict[index][key]:\n",
    "                        predict_dict[key] *= poste_dict[index][key][att]\n",
    "                    else:\n",
    "                        # Epsilon smoothing if no value exists\n",
    "                        predict_dict[key] *= 0.01 / sum_value\n",
    "        # Append the class with most possible probability value\n",
    "        predict_list.append(max(predict_dict, key=predict_dict.get))\n",
    " \n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation` : This functions makes predictions on the model and returns the predicted list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "Now, that everything has been done, Mayuri asks you to make a function which takes the data list, finename and the predicted data as arguments and prints the confusion matrix for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context\n",
    "def evaluate_supervised(data, filename, predict_list):\n",
    "    \"\"\"Evaluate the predictions for supervised NB\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: 2-D array of data from csv file\n",
    "    predict_list: list of predicted classes\n",
    "    \"\"\"\n",
    "    print('{0:*^105}'.format('supervised' + ' ' + filename.split('.')[0]))\n",
    "    # Print confusion matrix\n",
    "    print_matrix(data, predict_list)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interpretation` This function prints the confusion matrix for the given file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "Now Finally, Mayuri asks you to make the function which will be the mega function to call all the required fucntions.\n",
    "\n",
    "`How`?\n",
    "\n",
    "This would take the files as arguments, remember we had created the file list earlier? \n",
    "Iterating through each file, it would call the `preprocess` function, the `train_supervised` function, the `predict_list` function and the `evaluate_supervised` fucntion.\n",
    "\n",
    "Atlast, the mega function will be called for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************supervised car**********************************************\n",
      "      Actual\\Predict|               unacc|                 acc|               vgood|                good|\n",
      "               unacc|                1161|                  47|                   0|                   2|\n",
      "                 acc|                  85|                 289|                   0|                  10|\n",
      "               vgood|                   0|                  26|                  39|                   0|\n",
      "                good|                   0|                  46|                   2|                  21|\n",
      "\n",
      "Accuracy: 0.8738425925925926\n",
      "\n",
      "*****************************************supervised hypothyroid******************************************\n",
      "      Actual\\Predict|         hypothyroid|            negative|\n",
      "         hypothyroid|                   0|                 151|\n",
      "            negative|                   0|                3012|\n",
      "\n",
      "Accuracy: 0.9522605121719886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main function for supervised NB\n",
    "def supervised(file_list):\n",
    "    \n",
    "    for filename in file_list:\n",
    "        data = preprocess(filename)\n",
    "        prior_dict, poste_dict = train_supervised(data)\n",
    "        predict_list = predict_supervised(data, prior_dict, poste_dict)\n",
    "        evaluate_supervised(data, filename, predict_list)\n",
    "        \n",
    "# Run the next line of code, can show the confusion matrix of all supervised dataset\n",
    "supervised(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Conclusion`:\n",
    "\n",
    "We can conclude that \n",
    "* The model is properly made since it is very accurate on both the files.\n",
    "* Naive Bayes primality requires understanding of the prior and posterior probabilities\n",
    "* Naive bayes is a supervised learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
