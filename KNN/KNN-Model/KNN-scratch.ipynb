{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "Your cousin brother Dev has been learning classification algorithms lately. He went through multiple tutorials over the implementation of the KNN algorithm but could not find very good material on it's implementation in Python without any data science libraries.\n",
    "\n",
    "So, he decides to try it on his own. He then seeks your help, since you are yourself a data science enthusiast.\n",
    "\n",
    "You have to now help in out in implementing the KNN from scratch and explain him the Maths behind the algorithm.\n",
    "\n",
    "Remember, Behind every great algorithm lies even greater mathematics. \n",
    "\n",
    "\n",
    "You begin by explaining Dev the nuances and the steps involved in the KNN implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a data **classification algorithm** that attempts to determine what group a data point is in by looking at the data points around it.\n",
    "\n",
    "An algorithm, looking at one point on a grid, trying to determine if a point is in group A or B, looks at the states of the points that are near it. The range is arbitrarily determined, but the point is to take a sample of the data. If the majority of the points are in group A, then it is likely that the data point in question will be A rather than B, and vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "K-Nearest Neighbors (KNN) is one of the simplest algorithms used in Machine Learning for regression and classification problem. KNN algorithms use data and classify new data points based on similarity measures (e.g. distance function). Classification is done by a majority vote to its neighbors\n",
    "We have some labeled data set, $X-train$, and a new set $X$ that we want to classify based on previous classifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You further explain Dev the steps that he would have to follow to implement the KNN.\n",
    "\n",
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the csv file. Note that here we will not be using Pandas for the same. We will try loading the file using plain file handling.\n",
    "#### 2. Do the basic cleansing of the data. (Convert String typed to integer types for better predictions and models.\n",
    "#### 3. Calculate distance to all neightbours (Eucledian Distance, Manhattan Distance and Minkowski distance)\n",
    "#### 4. Spiltting Dataset into train test fold, and then cross validating.\n",
    "#### 5. Create classification model. Calculate accuracy and confusion matrix.\n",
    "#### 6. Sort neighbours (based on closest distance)\n",
    "#### 7. Use the model on real data. Here we will use the IRIS dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the first step to any program is to import the libraries.\n",
    "# Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from random import randrange\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "Now, you ask Dev to define a function to load the csv file.\n",
    "He is confused as he knows that files can easily be loaded using the Pandas library. So, you tell him to use the basic file handling methods to read the csv file.\n",
    "\n",
    "One advantage of reading datasets like this is that you can easily skip empty rows, as we would see below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Here, you define a function load_csv which takes the file as an argument.\n",
    "\n",
    "-The dataset is created as a list.\n",
    "\n",
    "-You open the file in read mode.\n",
    "\n",
    "-As we have imported the csv package, we use csv reader function to read the csv file.\n",
    "\n",
    "-We additionally check for empty rows and omit them.\n",
    "\n",
    "-Return the dataset\n",
    "\n",
    "Hint: Import csv and use **csv.reader** to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename): #define a function to load the file\n",
    "    dataset = list() #define a list where you will store the data\n",
    "    with open(filename, 'r') as file: #open file in read mode\n",
    "        csv_reader = csv.reader(file) #use csv.reader to load the file\n",
    "        for row in csv_reader: #iterate through every row\n",
    "            if not row: #skip if it's an empty row\n",
    "                continue\n",
    "            dataset.append(row) #if not empty, append data of each row in the list dataset.\n",
    "    return dataset #return thr dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "Now you ask Dev if he is familiar with data cleansing techniques. When he says that he knows what data cleansing and preprocessing is, you ask him the importance of converting string to integer type in data science. \n",
    "\n",
    "\n",
    "He answers that The model cannot process string type data and hence you need to convert it to it's corresponsind numeric data for better predictions and accuracy.\n",
    "\n",
    "Dev then codes the conversion function of both converting strings to float and integers, since we do not know what conversion would the data set require.\n",
    "\n",
    "\n",
    "Hint : The **strip()** method returns a copy of the string by removing both the leading and the trailing characters (based on the string argument passed). The strip() method removes characters from both left and right based on the argument (a string specifying the set of characters to be removed).\n",
    "\n",
    "Eg: \n",
    "\n",
    "    txt = \"     banana     \"\n",
    "    x = txt.strip()\n",
    "(x will be \"banana\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string column to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str_column_to_float(dataset, column): #take the dataset and the column to be converted as argument\n",
    "    for row in dataset: #iterate through each row\n",
    "        row[column] = float(row[column].strip()) #convert to float\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string column to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How?`\n",
    "\n",
    "Hint: `set()` method is used to convert any of the iterable to sequence of iterable elements with distinct elements, commonly called Set. Parameters : Any iterable sequence like list, tuple or dictionary. Returns : An empty set if no element is passed.\n",
    "\n",
    "Eg:\n",
    "\n",
    "    set_example = {1, 2, 3, 4, 5, 5, 5}\n",
    "    print(set_example)\n",
    "\n",
    "OUTPUT:\n",
    "{1, 2, 3, 4, 5} ----- Does not print repetitions\n",
    "\n",
    "\n",
    "Hint: `enumerate()` method adds a counter to an iterable and returns it in a form of enumerate object. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str_column_to_int(dataset, column): #take the dataset and oolumn to be converted as argument\n",
    "    class_values = [row[column] for row in dataset] #iterate through each cell\n",
    "    unique = set(class_values) #unique is a list of distinct data\n",
    "    lookup = dict() #create a dictionary to store the mapping\n",
    "    for i, value in enumerate(unique): \n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "Now we move to defining the different distances that we would require for our classification.\n",
    "You explain Dev the differences between the following types of distances:\n",
    "\n",
    "### Euclidean Distance:\n",
    "\n",
    " Euclidean distance is the \"ordinary\" straight-line distance between two points in D-Dimensional space\n",
    "\n",
    "#### Definiton\n",
    "$d(p, q) = d(q, p) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + \\dots + (q_D - p_D)^2} = \\sum_{d=1}^{D} (p_d - q_d)^2$\n",
    "\n",
    "#### Example\n",
    "Distance in $R^2$\n",
    "\n",
    "\n",
    "$p = (4,6)$\n",
    "\n",
    "$q = (1,2)$\n",
    "\n",
    "$d(p, q) = \\sqrt{(1-4)^2 + (2-6)^2} =\\sqrt{9 + 16} = \\sqrt{25} = 5 $\n",
    "\n",
    "\n",
    "### Manhattan Distance:\n",
    "\n",
    "The distance between two points measured along axes at right angles. In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is |x1 - x2| + |y1 - y2|. \n",
    "\n",
    "### Minkowski distance\n",
    "\n",
    "The Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance. It is named after the German mathematician Hermann Minkowski.\n",
    "The Minkowski distance between two variabes X and Y is defined as\n",
    "\n",
    "    (∑i=1n|Xi−Yi|p)1/p\n",
    "\n",
    "The case where p = 1 is equivalent to the Manhattan distance and the case where p = 2 is equivalent to the Euclidean distance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the theory is clear, you ask Dev to define the mathematical python functions for the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you explain Dev over how to code the Eucledian Distance function.\n",
    "\n",
    "The training and testing sets are passed as parameters, along with the length. Then, the distance  is calculated using the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += pow(instance2[i]-instance1[i],2) #applying the formula\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ManhattanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += abs(instance2[i]-instance1[i]) #applying the formula\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Minkowski distance with parameter p for power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinkowskiDistance(instance1, instance2, length, p):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += pow(abs(instance2[i]-instance1[i]), p) #applying the formula\n",
    "    return pow(distance, 1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset methods \n",
    "\n",
    "`Story`\n",
    "\n",
    "You explain Dev the process of splitting data, and hence define the function train_test_split.\n",
    "We know that we require a split ratio, which we will enter manually.\n",
    "\n",
    "We take the dataset and split as the arguments for the function, and then append the (rows * split) ubt train list.\n",
    "\n",
    "We then return the train list and the original dataset list.\n",
    "\n",
    "Hint: The training dataset size would be the split parameter multiplied by the lenghth of the dataset, since the split parameter is nothing but the spplit ratio.\n",
    "\n",
    "Then, we will make another dataset which will be the training dataset, of size train size.\n",
    "\n",
    "\n",
    "Hint: The `append()` method in python adds a single item to the existing list. It doesn't return a new list of items but will modify the original list by adding the item to the end of the list.\n",
    "\n",
    "`pop()` is an inbuilt function in Python that removes and returns last value from the list or the given index value. Parameter : index (optional) - The value at index is popped out and removed. If the index is not given, then the last element is popped out and removed.\n",
    "\n",
    "The `randrange()` method returns a randomly selected element from the specified range.\n",
    "### Split a dataset into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(dataset, split): #argument is dataset and splitfactor ( lies betweem 0.1 to 0.9)\n",
    "    train = list() #make a list\n",
    "    train_size = split * len(dataset) # the size of the training set\n",
    "    dataset_copy = list(dataset) # you copy the entire dataset\n",
    "    while len(train) < train_size: #create dataset of training size\n",
    "        index = randrange(len(dataset_copy)) \n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "You now explain Dev about `Cross Validation` and `splitting the data into K folds`.\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
    "\n",
    "It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n",
    "\n",
    "**k-Fold Cross-Validation**\n",
    "\n",
    "Cross-validation is a **resampling** procedure used to evaluate machine learning models on a limited data sample.\n",
    "Resampling is the method that consists of drawing repeated samples from the original data samples. The method of Resampling is a nonparametric method of statistical inference. ... The method of resampling uses experimental methods, rather than analytical methods, to generate the unique sampling distribution.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "`How?`\n",
    "The general procedure is as follows:\n",
    "\n",
    "    Shuffle the dataset randomly.\n",
    "    Split the dataset into k groups\n",
    "    For each unique group:\n",
    "        Take the group as a hold out or test data set\n",
    "        Take the remaining groups as a training data set\n",
    "        Fit a model on the training set and evaluate it on the test set\n",
    "        Retain the evaluation score and discard the model\n",
    "    Summarize the skill of the model using the sample of model evaluation scores\n",
    "    \n",
    "Let's now define the function for splitting data for k folds.\n",
    "\n",
    "**We take the dataset and the number of folds as arguments, and then follow the procedure**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a dataset into $k$ folds\n",
    "\n",
    "`How?`\n",
    "Here, we take the dataset and number of folds as function arguments, and then create a list.\n",
    "The foldsize is calculated by the ratio of the length of dataset and the number of folds.\n",
    "Accordingly, a new dataset is created after folding.\n",
    "\n",
    "\n",
    "Dev now begins to understanding the functioning of the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, folds):\n",
    "    dataset_split = list() #create a list\n",
    "    dataset_copy = list(dataset) #copy the dataset\n",
    "    fold_size = int(len(dataset) / folds) #the size of fold will be number of rows divided by the fold factor\n",
    "    for i in range(folds):\n",
    "        fold = list() #create a list\n",
    "        while len(fold) < fold_size: #split the dataset\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Dev is very excited since the model is about to be built and only defining the neighbour function remains.\n",
    "\n",
    "However, you first tell him to code the accuracy functions, and description functions first.\n",
    "\n",
    "The description functions would consist of the confusion matrix, and the classification estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for classification problems \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How?`\n",
    "\n",
    "You pass the testSet and the prediction as the arguments, check if the values are the same and then return the accuracy.\n",
    "\n",
    "We compare the data of the predictions to the actual data from the training  set, and then calculate the accuracy.\n",
    "### Get accuracy of prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(actual,predicted): #take the actual and the predicted value as arguments\n",
    "    correct = 0 #initialise with 0\n",
    "    for i in range(len(actual)): #iterate throw rows\n",
    "        if actual[i][-1] == predicted[i]: #if equal, then increase the correct counter by 1\n",
    "            correct += 1\n",
    "    return (correct / float(len(actual))) * 100.00 # return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "Now you ask Dev if he knows what a `confusion matrix` is. He replies in the negative.\n",
    "\n",
    "You move on to explain him that in the field of data science and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one.\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now create the function for the confusion matrix, where actual and the predicted values are passed as arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate a Confusion Matrix \n",
    "\n",
    "`How?`\n",
    "\n",
    "Here, we take the actual and predicted values as arguments ( since this too, is accuracy model).\n",
    "\n",
    "Now, we define *unique* and *matrix* as variables with values set accordingly.\n",
    "\n",
    "We have explained Dev what do enumerate and set functions do already, so this won't be too difficukt for him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set([row[-1] for row in actual]) \n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i][-1]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[x][y] += 1\n",
    "    return unique, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Printing a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(unique, matrix):\n",
    "    print('Unique prediction values:')\n",
    "    print('(P)' + ' '.join(str(x) for x in unique))\n",
    "    print('(A)---')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for i, x in enumerate(unique):\n",
    "        print(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you move ahead to clear more of Dev's concepts and explain him the Precision Recall and F1 score values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n",
    "\n",
    "**Precision:** Also called Positive predictive value. The ratio of correct positive predictions to the total predicted positives. Recall — Also called Sensitivity, Probability of Detection, True Positive Rate. The ratio of correct positive predictions to the total positives examples.\n",
    "\n",
    "**Recall:** Also called Sensitivity, Probability of Detection, True Positive Rate. The ratio of correct positive predictions to the total positives examples.\n",
    "\n",
    "**The F1 score**, also known as balanced F-score or F-measure. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall classification estimator \n",
    "\n",
    "The matrix you created as the confusion matrix will be taken as argument, and then the numerical values of the recall precision and f1 score will be calculated using the formuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_precision_calc(matrix):\n",
    "    for i in range(len(matrix[0])):\n",
    "        row_values = matrix[i] # row values of matrix\n",
    "        col_values = [row[i] for row in matrix] # column values of matrix\n",
    "        tp = col_values[i]\n",
    "        fp = sum(row_values)-row_values[i] # sum all row values - ones in diagonal\n",
    "        fn = sum(col_values)-col_values[i] # sum all col values - ones in diagonal\n",
    "    \n",
    "    recall = tp / (tp + fn) #formulae\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return recall, precision, F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Story`\n",
    "\n",
    "Now you finally ask Dev to define the getNeighbour function which takes the training set, instance , the number of neighbours and the type of distance are arguments and return the neighbours (distances).\n",
    "\n",
    "Further we define a function which takes these neighbours as argument and classifies from the neighbours data.\n",
    "\n",
    "First, a list is created which is meant to store thr distances of the training sets.\n",
    "\n",
    "`How?`\n",
    "\n",
    "Here, we take the training set, testing instance, number of neighbours, and the type of distance to be used as arguments. Note that there is an extra argument given since the Minowski distance requires a power factor P as well. So in case Minowski distance is used, we have to consider the P factor in mind.\n",
    "\n",
    "Now, we check for the type of distance and then calculate the distance between the neighbours using the functions we had created earlier. (Remember?)\n",
    "\n",
    "The distance list is finally sorted.\n",
    "A list which stores the neighbours is created and the distances are appended to the neighbours, until the number of neighbours defined are itrerated upon.\n",
    "\n",
    "Let's code it now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Get neighbors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getNeighbors(trainingSet, testInstance, num_neighbors, distancetype, *args):\n",
    "    distances = [] #To store distance\n",
    "    length = len(testInstance)-1\n",
    "    for i in range(len(trainingSet)): #iterate through the training set\n",
    "        if distancetype == \"Euclidean\":\n",
    "            dist = EuclideanDistance(testInstance, trainingSet[i], length) #type of distance calcualted\n",
    "        elif distancetype == \"Manhattan\":\n",
    "            dist = ManhattanDistance(testInstance, trainingSet[i], length)\n",
    "        else:\n",
    "            dist = MinkowskiDistance(testInstance, trainingSet[i], length, *args)\n",
    "        distances.append((trainingSet[i],dist)) #append the distances in the list for all instances\n",
    "    distances.sort(key=operator.itemgetter(1)) #sort them\n",
    "    #return distances\n",
    "    neighbors = [] #create an empty list\n",
    "    for x in range(num_neighbors): #iterate through number of neighbours\n",
    "        neighbors.append(distances[x][0]) #store the minimum distance for each iteration\n",
    "    return neighbors #return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification from neighbors (Classification problem)\n",
    "\n",
    "In this function we store the response and count the no. of votes for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hint`\n",
    "NOTE: **operator.itemgetter(n)** constructs a callable that assumes an iterable object (e.g. list, tuple, set) as input, and fetches the n-th element out of it.\n",
    "\n",
    "Here, we will create a tuple which counts the number of neighbours of a particular type (classifies them), and the type having the highest count is returned.\n",
    "\n",
    "A counter is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(neighbors): #take the neighbors returned from the above cell as argument\n",
    "    classVotes = {} #create a tuple\n",
    "    for x in range(len(neighbors)): #iteration\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1 #if response is present then add the classvotes counter\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True) #sort in descending order\n",
    "    return sortedVotes[0][0] #return with highest number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now explain Dev that all the functions are ready and it's time to call them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How`\n",
    "We start by loading the **dataset IRIS** and calling the load_csv function.\n",
    "\n",
    "Then we check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file iris.csv with 150 rows and 5 columns\n",
      "First line of dataset:  ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename) #call function\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "print('First line of dataset: ', dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have string column which require to be converted to int and float. We will convert the first four columns to plain float and the last class column to 0,1,2 respectively donating the type of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Convert string columns to float** using the function we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    str_column_to_float(dataset, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert class column to int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of dataset with class defined by integer:  [5.1, 3.5, 1.4, 0.2, 0]\n",
      "\n",
      "Dictionary of lookup classes:  {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup = str_column_to_int(dataset, 4)\n",
    "print('First line of dataset with class defined by integer: ', dataset[0])\n",
    "print('')\n",
    "print('Dictionary of lookup classes: ', lookup)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting dataset between Training and Testing Set. \n",
    "Here, we can use any value of split from 0.1 to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.6\n",
    "trainingSet, testSet = train_test_split(dataset, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate predictions and show the output.**\n",
    "\n",
    "We store the predictions in  a list, and define the number of neighbours to be any number we want. Then we run the model we built across the range of the testset length, where we call both **getneighbours** and **getresponse** function.\n",
    "\n",
    "We store the response in another variable and we append it to the predictions list at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm solving:\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=2, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=2, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=1, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n"
     ]
    }
   ],
   "source": [
    "print('Algorithm solving:')\n",
    "predictions = []\n",
    "num_neighbors = 3\n",
    "for i in range(len(testSet)):\n",
    "    neighbors = getNeighbors(trainingSet, testSet[i], num_neighbors, \"Euclidean\")\n",
    "    classify = getResponse(neighbors)\n",
    "    predictions.append(classify)\n",
    "    print('> predicted=' + repr(classify) + ', actual=' + repr(testSet[i][-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :95.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy = getAccuracy(testSet,predictions)\n",
    "print('Accuracy :' + repr(accuracy) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unique prediction values:\n",
      "(P)0 1 2\n",
      "(A)---\n",
      "Confusion Matrix:\n",
      "0| 12 0 0\n",
      "1| 0 22 2\n",
      "2| 0 1 23\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique, matrix = confusion_matrix(testSet, predictions)\n",
    "print('\\n')\n",
    "print_confusion_matrix(unique, matrix)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate properties for recall and precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.92\n",
      "Precision: 0.9583333333333334\n",
      "F1 score: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "Recall, Precision, F1_score = recall_precision_calc(matrix)\n",
    "print('Recall:', Recall)\n",
    "print('Precision:', Precision)\n",
    "print('F1 score:', F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: We see how Dev has built the mathematical model for KNN algorithm classification from scratch in Python.\n",
    "\n",
    "For the IRIS dataset in Eucledian distance, the accuracy is 95%.\n",
    "We can change the dataset and the type of distance to explore our model more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
